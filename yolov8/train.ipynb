{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.train(data=\"../data.yaml\", epochs=2, imgsz=640, device='mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with freezing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done with the help of custom callback functions in Ultralytics. See the link for further reference:\n",
    "https://docs.ultralytics.com/usage/callbacks/#trainer-callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.weight\n",
      "model.0.bn.weight\n",
      "model.0.bn.bias\n",
      "model.1.conv.weight\n",
      "model.1.bn.weight\n",
      "model.1.bn.bias\n",
      "model.2.cv1.conv.weight\n",
      "model.2.cv1.bn.weight\n",
      "model.2.cv1.bn.bias\n",
      "model.2.cv2.conv.weight\n",
      "model.2.cv2.bn.weight\n",
      "model.2.cv2.bn.bias\n",
      "model.2.m.0.cv1.conv.weight\n",
      "model.2.m.0.cv1.bn.weight\n",
      "model.2.m.0.cv1.bn.bias\n",
      "model.2.m.0.cv2.conv.weight\n",
      "model.2.m.0.cv2.bn.weight\n",
      "model.2.m.0.cv2.bn.bias\n",
      "model.3.conv.weight\n",
      "model.3.bn.weight\n",
      "model.3.bn.bias\n",
      "model.4.cv1.conv.weight\n",
      "model.4.cv1.bn.weight\n",
      "model.4.cv1.bn.bias\n",
      "model.4.cv2.conv.weight\n",
      "model.4.cv2.bn.weight\n",
      "model.4.cv2.bn.bias\n",
      "model.4.m.0.cv1.conv.weight\n",
      "model.4.m.0.cv1.bn.weight\n",
      "model.4.m.0.cv1.bn.bias\n",
      "model.4.m.0.cv2.conv.weight\n",
      "model.4.m.0.cv2.bn.weight\n",
      "model.4.m.0.cv2.bn.bias\n",
      "model.4.m.1.cv1.conv.weight\n",
      "model.4.m.1.cv1.bn.weight\n",
      "model.4.m.1.cv1.bn.bias\n",
      "model.4.m.1.cv2.conv.weight\n",
      "model.4.m.1.cv2.bn.weight\n",
      "model.4.m.1.cv2.bn.bias\n",
      "model.5.conv.weight\n",
      "model.5.bn.weight\n",
      "model.5.bn.bias\n",
      "model.6.cv1.conv.weight\n",
      "model.6.cv1.bn.weight\n",
      "model.6.cv1.bn.bias\n",
      "model.6.cv2.conv.weight\n",
      "model.6.cv2.bn.weight\n",
      "model.6.cv2.bn.bias\n",
      "model.6.m.0.cv1.conv.weight\n",
      "model.6.m.0.cv1.bn.weight\n",
      "model.6.m.0.cv1.bn.bias\n",
      "model.6.m.0.cv2.conv.weight\n",
      "model.6.m.0.cv2.bn.weight\n",
      "model.6.m.0.cv2.bn.bias\n",
      "model.6.m.1.cv1.conv.weight\n",
      "model.6.m.1.cv1.bn.weight\n",
      "model.6.m.1.cv1.bn.bias\n",
      "model.6.m.1.cv2.conv.weight\n",
      "model.6.m.1.cv2.bn.weight\n",
      "model.6.m.1.cv2.bn.bias\n",
      "model.7.conv.weight\n",
      "model.7.bn.weight\n",
      "model.7.bn.bias\n",
      "model.8.cv1.conv.weight\n",
      "model.8.cv1.bn.weight\n",
      "model.8.cv1.bn.bias\n",
      "model.8.cv2.conv.weight\n",
      "model.8.cv2.bn.weight\n",
      "model.8.cv2.bn.bias\n",
      "model.8.m.0.cv1.conv.weight\n",
      "model.8.m.0.cv1.bn.weight\n",
      "model.8.m.0.cv1.bn.bias\n",
      "model.8.m.0.cv2.conv.weight\n",
      "model.8.m.0.cv2.bn.weight\n",
      "model.8.m.0.cv2.bn.bias\n",
      "model.9.cv1.conv.weight\n",
      "model.9.cv1.bn.weight\n",
      "model.9.cv1.bn.bias\n",
      "model.9.cv2.conv.weight\n",
      "model.9.cv2.bn.weight\n",
      "model.9.cv2.bn.bias\n",
      "model.12.cv1.conv.weight\n",
      "model.12.cv1.bn.weight\n",
      "model.12.cv1.bn.bias\n",
      "model.12.cv2.conv.weight\n",
      "model.12.cv2.bn.weight\n",
      "model.12.cv2.bn.bias\n",
      "model.12.m.0.cv1.conv.weight\n",
      "model.12.m.0.cv1.bn.weight\n",
      "model.12.m.0.cv1.bn.bias\n",
      "model.12.m.0.cv2.conv.weight\n",
      "model.12.m.0.cv2.bn.weight\n",
      "model.12.m.0.cv2.bn.bias\n",
      "model.15.cv1.conv.weight\n",
      "model.15.cv1.bn.weight\n",
      "model.15.cv1.bn.bias\n",
      "model.15.cv2.conv.weight\n",
      "model.15.cv2.bn.weight\n",
      "model.15.cv2.bn.bias\n",
      "model.15.m.0.cv1.conv.weight\n",
      "model.15.m.0.cv1.bn.weight\n",
      "model.15.m.0.cv1.bn.bias\n",
      "model.15.m.0.cv2.conv.weight\n",
      "model.15.m.0.cv2.bn.weight\n",
      "model.15.m.0.cv2.bn.bias\n",
      "model.16.conv.weight\n",
      "model.16.bn.weight\n",
      "model.16.bn.bias\n",
      "model.18.cv1.conv.weight\n",
      "model.18.cv1.bn.weight\n",
      "model.18.cv1.bn.bias\n",
      "model.18.cv2.conv.weight\n",
      "model.18.cv2.bn.weight\n",
      "model.18.cv2.bn.bias\n",
      "model.18.m.0.cv1.conv.weight\n",
      "model.18.m.0.cv1.bn.weight\n",
      "model.18.m.0.cv1.bn.bias\n",
      "model.18.m.0.cv2.conv.weight\n",
      "model.18.m.0.cv2.bn.weight\n",
      "model.18.m.0.cv2.bn.bias\n",
      "model.19.conv.weight\n",
      "model.19.bn.weight\n",
      "model.19.bn.bias\n",
      "model.21.cv1.conv.weight\n",
      "model.21.cv1.bn.weight\n",
      "model.21.cv1.bn.bias\n",
      "model.21.cv2.conv.weight\n",
      "model.21.cv2.bn.weight\n",
      "model.21.cv2.bn.bias\n",
      "model.21.m.0.cv1.conv.weight\n",
      "model.21.m.0.cv1.bn.weight\n",
      "model.21.m.0.cv1.bn.bias\n",
      "model.21.m.0.cv2.conv.weight\n",
      "model.21.m.0.cv2.bn.weight\n",
      "model.21.m.0.cv2.bn.bias\n",
      "model.22.cv2.0.0.conv.weight\n",
      "model.22.cv2.0.0.bn.weight\n",
      "model.22.cv2.0.0.bn.bias\n",
      "model.22.cv2.0.1.conv.weight\n",
      "model.22.cv2.0.1.bn.weight\n",
      "model.22.cv2.0.1.bn.bias\n",
      "model.22.cv2.0.2.weight\n",
      "model.22.cv2.0.2.bias\n",
      "model.22.cv2.1.0.conv.weight\n",
      "model.22.cv2.1.0.bn.weight\n",
      "model.22.cv2.1.0.bn.bias\n",
      "model.22.cv2.1.1.conv.weight\n",
      "model.22.cv2.1.1.bn.weight\n",
      "model.22.cv2.1.1.bn.bias\n",
      "model.22.cv2.1.2.weight\n",
      "model.22.cv2.1.2.bias\n",
      "model.22.cv2.2.0.conv.weight\n",
      "model.22.cv2.2.0.bn.weight\n",
      "model.22.cv2.2.0.bn.bias\n",
      "model.22.cv2.2.1.conv.weight\n",
      "model.22.cv2.2.1.bn.weight\n",
      "model.22.cv2.2.1.bn.bias\n",
      "model.22.cv2.2.2.weight\n",
      "model.22.cv2.2.2.bias\n",
      "model.22.cv3.0.0.conv.weight\n",
      "model.22.cv3.0.0.bn.weight\n",
      "model.22.cv3.0.0.bn.bias\n",
      "model.22.cv3.0.1.conv.weight\n",
      "model.22.cv3.0.1.bn.weight\n",
      "model.22.cv3.0.1.bn.bias\n",
      "model.22.cv3.0.2.weight\n",
      "model.22.cv3.0.2.bias\n",
      "model.22.cv3.1.0.conv.weight\n",
      "model.22.cv3.1.0.bn.weight\n",
      "model.22.cv3.1.0.bn.bias\n",
      "model.22.cv3.1.1.conv.weight\n",
      "model.22.cv3.1.1.bn.weight\n",
      "model.22.cv3.1.1.bn.bias\n",
      "model.22.cv3.1.2.weight\n",
      "model.22.cv3.1.2.bias\n",
      "model.22.cv3.2.0.conv.weight\n",
      "model.22.cv3.2.0.bn.weight\n",
      "model.22.cv3.2.0.bn.bias\n",
      "model.22.cv3.2.1.conv.weight\n",
      "model.22.cv3.2.1.bn.weight\n",
      "model.22.cv3.2.1.bn.bias\n",
      "model.22.cv3.2.2.weight\n",
      "model.22.cv3.2.2.bias\n",
      "model.22.dfl.conv.weight\n"
     ]
    }
   ],
   "source": [
    "for k, v in yolo.model.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layer(trainer):\n",
    "    model = trainer.model\n",
    "    num_freeze = 10\n",
    "    print(f\"Freezing {num_freeze} layers\")\n",
    "    freeze = [f'model.{x}.' for x in range(num_freeze)]  # layers to freeze \n",
    "    for k, v in model.named_parameters(): \n",
    "        v.requires_grad = True  # train all layers \n",
    "        if any(x in k for x in freeze): \n",
    "            print(f'freezing {k}') \n",
    "            v.requires_grad = False \n",
    "    print(f\"{num_freeze} layers are freezed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.add_callback(\"on_train_start\", freeze_layer)\n",
    "yolo.train(data=\"../data.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing layers in YOLOv8 using a custom callback function can indeed help to freeze specific layers during training. However, freezing layers may not necessarily result in faster training speed.\n",
    "\n",
    "Freezing layers typically aims to retain the knowledge in the pre-trained layers while only updating the unfrozen layers. This can be useful when you want to fine-tune a model on a new dataset without losing the previously learned knowledge.\n",
    "\n",
    "The reason why freezing layers may not lead to faster training speed in YOLOv8 could be due to the nature of the model architecture. YOLOv8 consists of multiple components, including backbone layers, neck layers, and detection heads. Freezing layers in one component may not necessarily speed up the training process since the model still needs to compute forward and backward passes through the unfrozen layers.\n",
    "\n",
    "It's important to note that the effectiveness of freezing layers may vary depending on the specific use case and dataset. It's always a good idea to experiment and evaluate the impact of freezing layers on the overall training performance and model accuracy in a particular scenario.\n",
    "\n",
    "However, freezing too many layers can cause the model to lose the ability to learn and make accurate predictions. Instead of freezing the first 10 layers, try freezing only a smaller number of layers, such as the first 3-5 layers. This can help the model to retain its ability to learn and improve accuracy on the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do transfer learning the right way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using YOLOv8, you can indeed use transfer learning by freezing the first few layers of the model. By freezing these layers, you can retain the pre-trained weights from a previous model (trained on dataset A) and fine-tune the model on a new dataset (dataset B).\n",
    "\n",
    "However, it's important to note that freezing the initial layers will only allow the model to focus on learning the patterns specific to dataset B. As a result, the model will primarily detect objects from dataset B and may not perform as well on objects from dataset A.\n",
    "\n",
    "In your case, when you load the bestA.pt weights and freeze the first 5 layers, the model will learn to detect objects specific to dataset B while ignoring the objects from dataset A. This behavior is expected as the model is fine-tuned to prioritize dataset B.\n",
    "\n",
    "If you want the model to detect objects from both datasets A and B, you would need to train the model on a combined dataset that includes samples from both datasets. This way, the model can learn to detect objects from both datasets simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In YOLOv8, lr0 and lrf are both used during training to control the learning rate schedule. The lr0 parameter is the initial learning rate, and lrf will calculate the final learning rate at the last epoch of training (=lr0 * lrf). By default, in YOLOv8, both lr0 and lrf have the same value (0.01), and this value is used during training. If you changed these values individually and observed no change in training behavior, then it is possible that other factors (such as the number of epochs or batch size) might be affecting the learning rate as well.\n",
    "\n",
    "Regarding the cos_lr parameter, if it is set to True, then the learning rate schedule will follow a cosine annealing pattern rather than a linear schedule. This can lead to a smoother learning rate schedule and potentially better results. Both lr0 and lrf are still used in the cosine annealing learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
